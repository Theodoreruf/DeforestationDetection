{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3ht-KC4Dahv"
      },
      "outputs": [],
      "source": [
        "\"\"\"satellites_unet.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/142XiiXOko5VDq93Y5DF3GKaOo6MoUclm\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import skimage.io\n",
        "import pandas as pd\n",
        "from pandas import array\n",
        "\n",
        "from tensorflow.keras.utils import normalize\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.optimizers import Adam\n",
        "import glob\n",
        "\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "\n",
        "from skimage.color import rgb2gray\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9O8eyytDajy"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Activation, MaxPool2D, Concatenate\n",
        "\n",
        "### Défintion des blocs ###\n",
        "\n",
        "# Bloc de convolution (2 conv and batch layers) #\n",
        "def conv_block(input, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)   #Not in the original network.\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)  #Not in the original network\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# Bloc de l'encoder #\n",
        "def encoder_block(input, num_filters):\n",
        "    x = conv_block(input, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "\n",
        "# Bloc du décoder #\n",
        "def decoder_block(input, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "# Structure/Assemblage des blocs --> de U-Net #\n",
        "def build_unet(input_shape, n_classes):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024) #Bridge\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    if n_classes == 1:\n",
        "      activation = 'sigmoid'\n",
        "    else:\n",
        "      activation = 'softmax'\n",
        "\n",
        "    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# my_unet = build_unet(input_shape=(256,256,3), n_classes=3)\n",
        "# my_unet.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWqoWTUIDalr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "SIZE = 256\n",
        "num_images = 1200\n",
        "\n",
        "# Data Paths\n",
        "ROOT_IMAGE_DIR = '/content/drive/MyDrive/Forest Segmented/images/'\n",
        "ROOT_MASK_DIR = '/content/drive/MyDrive/Forest Segmented/masks/'\n",
        "METADATA_CSV_PATH = '/content/drive/MyDrive/Forest Segmented/meta_data.csv'\n",
        "\n",
        "# Read the images\n",
        "arrayed_images = []\n",
        "arrayed_masks = []\n",
        "\n",
        "\n",
        "image_names = glob.glob(\"/content/drive/MyDrive/Forest Segmented/images/*.jpg\")\n",
        "image_names_subset = image_names[0:num_images]\n",
        "print(image_names[0])\n",
        "\n",
        "\n",
        "for i,img in enumerate(image_names_subset):\n",
        "    img = Image.open(img)\n",
        "    array_img = np.array(img)\n",
        "    arrayed_images.append(array_img)\n",
        "\n",
        "mask_names = glob.glob(\"/content/drive/MyDrive/Forest Segmented/masks/*.jpg\")\n",
        "mask_names_subset = mask_names[0:num_images]\n",
        "\n",
        "\n",
        "for i,img in enumerate(mask_names_subset):\n",
        "    img = Image.open(img)\n",
        "    array_img = np.array(img)\n",
        "    arrayed_masks.append(array_img)\n",
        "\n",
        "import csv\n",
        "# Chargement des correspondances depuis le fichier CSV\n",
        "correspondances = {}\n",
        "with open('/content/drive/MyDrive/Forest Segmented/meta_data.csv', 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for i,row in enumerate(reader):\n",
        "        correspondances[row[0]] = row[1]\n",
        "\n",
        "\n",
        "print(correspondances)\n",
        "\n",
        "# Création de deux nouvelles listes triées en fonction des correspondances\n",
        "sorted_images = []\n",
        "sorted_masks = []\n",
        "for i, image in enumerate(image_names_subset):\n",
        "    image = image.split(\"/\")[-1]\n",
        "    sorted_images.append(image)\n",
        "    sorted_masks.append(correspondances[image])\n",
        "    # if i == 99 :\n",
        "    #   break;\n",
        "\n",
        "X_predictions = []\n",
        "Y_predictions = []\n",
        "arrayed_sorted_images = []\n",
        "for i,img in enumerate(sorted_images):\n",
        "    img = ROOT_IMAGE_DIR + img\n",
        "    img = Image.open(img)\n",
        "    array_img = np.array(img)\n",
        "    #array_img = cv2.cvtColor(array_img, cv2.COLOR_BGR2GRAY)\n",
        "    array_img = rgb2gray(img)\n",
        "    if i < 1000 :\n",
        "      arrayed_sorted_images.append(array_img)\n",
        "    else :\n",
        "      X_predictions.append(array_img)\n",
        "\n",
        "arrayed_sorted_masks = []\n",
        "for i,img in enumerate(sorted_masks):\n",
        "    img = ROOT_MASK_DIR + img\n",
        "    img = Image.open(img)\n",
        "    array_img = np.array(img)\n",
        "    #array_img = cv2.cvtColor(array_img, cv2.COLOR_BGR2GRAY)\n",
        "    array_img = rgb2gray(img)\n",
        "    if i < 1000 :\n",
        "      arrayed_sorted_masks.append(array_img)\n",
        "    else :\n",
        "      Y_predictions.append(array_img)\n",
        "\n",
        "\n",
        "dataset = np.expand_dims(arrayed_sorted_images, axis = 3)\n",
        "maskset = np.expand_dims(arrayed_sorted_masks, axis = 3)\n",
        "predictionset_x = np.expand_dims(X_predictions, axis = 3)\n",
        "predictionset_y = np.expand_dims(Y_predictions, axis = 3)\n",
        "\n",
        "print(\"Image data shape is: \", dataset.shape)\n",
        "print(\"mask data shape is: \", maskset.shape)\n",
        "print(\"prediction Image data shape is: \", predictionset_x.shape)\n",
        "print(\"prediction mask data shape is: \", predictionset_y.shape)\n",
        "print(\"Max pixel value in image is: \", dataset.max())\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, maskset, test_size = 0.2, random_state = 42)\n",
        "\n",
        "\n",
        "# --- MODELE UNET, TRAINING DAY --- #\n",
        "\n",
        "mean_iou = MeanIoU(num_classes=2, name=\"MeanIoU\")\n",
        "\n",
        "my_unet = build_unet(input_shape=(256,256,1), n_classes=1)\n",
        "my_unet.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=[mean_iou,'accuracy'])\n",
        "print(my_unet.summary())\n",
        "\n",
        "\n",
        "history = my_unet.fit(X_train, y_train,\n",
        "                    batch_size = 1,\n",
        "                    verbose= 1,\n",
        "                    epochs= 20,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    shuffle=False)\n",
        "\n",
        "#Save the model for future use\n",
        "print('training done, saving the model')\n",
        "#my_unet.save('/content/drive/MyDrive/Forest Segmented/sauvegarde_1200_binary_crossentropy_train_withDropout_butWithoutShuffle_20epochs.hdf5')\n",
        "\n",
        "#Load previously saved model\n",
        "from keras.models import load_model\n",
        "#my_unet = load_model(\"/content/drive/MyDrive/Forest Segmented/sauvegarde_1200_binary_crossentropy_train_withDropout_butWithoutShuffle_50epochs.hdf5\", compile=False)\n",
        "\n",
        "# plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'y', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "meaniou = history.history['MeanIoU']\n",
        "val_meaniou = history.history['val_MeanIoU']\n",
        "plt.plot(epochs, meaniou, 'y', label='Training meaniou')\n",
        "plt.plot(epochs, val_meaniou, 'r', label='Validation meaniou')\n",
        "plt.title('Training and validation meaniou')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('meaniou')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#IOU\n",
        "y_pred=my_unet.predict(predictionset_x)\n",
        "y_pred_thresholded = y_pred > 0.5 # assigner une valeur binaire à chaque pixel d'une image en fonction d'une valeur de seuil, ici 0.5 => x<0.5 -> 0 et x>0.5 ->1\n",
        "\n",
        "\n",
        "n_classes = 2\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)\n",
        "IOU_keras.update_state(y_pred_thresholded, y_test)\n",
        "#print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
        "\n",
        "\n",
        "threshold = 0.5\n",
        "test_img_number = random.randint(0, len(predictionset_x)-1)\n",
        "\n",
        "# test_img = X_test[test_img_number]\n",
        "# ground_truth=y_test[test_img_number]\n",
        "test_img = predictionset_x[25]\n",
        "ground_truth=predictionset_y[25]\n",
        "\n",
        "# test_img = predictionset_x[test_img_number]\n",
        "# ground_truth= predictionset_y[test_img_number]\n",
        "test_img_input=np.expand_dims(test_img, 0)\n",
        "#print(test_img_input.shape)\n",
        "prediction = (my_unet.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
        "print(\"Random prediction image has shape :\" ,prediction.shape, \"and has Mean IoU =\", IOU_keras.result().numpy())\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img)\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(ground_truth)\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(prediction, cmap='gray')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}